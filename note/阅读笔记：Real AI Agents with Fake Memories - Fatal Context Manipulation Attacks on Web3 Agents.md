![[../paper/AI safety/2025-Fatal Context Manipulation Attacks on Web3 Agents.pdf]]

# Abstract

 该摘要中实现的各项工作可总结如下：
1. **提出核心攻击向量概念**：引入“上下文操纵”这一综合性攻击向量，其利用未受保护的上下文表面（包括输入通道、内存模块、外部数据馈送），是对传统提示注入的扩展，并揭示了更隐蔽且持久的“内存注入”威胁。
2. **实验验证攻击影响**：借助ElizaOS（一个具有代表性的自动化Web3操作的去中心化AI代理框架），展示了对提示或历史记录的恶意注入可能引发未授权资产转移、协议违规等现实中具有毁灭性财务影响的后果。
3. **构建评估基准**：推出CrAIBench这一以Web3为重点的基准，涵盖150多个现实区块链任务（如代币转移、交易、跨链桥接、跨链交互等）以及500多个采用上下文操纵的攻击测试案例，用于量化相关风险。
4. **开展评估与分析**：通过评估证实，相比提示注入，AI模型对内存注入明显更脆弱；同时对一套全面的防御方案进行评估，发现当存储的上下文受损时，提示注入防御和检测器的保护作用有限，而基于微调的防御措施能在大幅降低攻击成功率的同时，保留在单步任务上的性能。
5. **强调需求与结论**：基于上述成果，强调在区块链环境中迫切需要既安全又负有信托责任的AI代理。

# Introduction

## contributions

- 上下文操纵攻击。
- 在 ElizaOS 上的实证验证。
- CrAIBench。
- 防御策略探索。

# Background and Related Work

## AI agents in decentralized finance (DeFi)

## Attacks on language agents

# Context Manipulation Attacks

## 形式化AI agent框架

![[attachments/Pasted image 20250722155909.png]]

# Case Study: Evaluating ElizaOS on Con-text Manipulation Attacks

## 持久化共享内存

ElizaOS 并非将每次对话或用户会话视为孤立的上下文，而是将完整的对话历史（包括会话、用户标识符、应用程序和单个消息）存储在外部数据库中实现持久化。

## 内存注入的脆弱性

ElizaOS实际上配备了标准的提示注入防护机制，能够检测并拦截直接注入恶意指令的行为。此外，由于每一次交互（无论成功还是被拦截）都会被记录在对话历史中，之后还会被检索到上下文里，因此，反复的提示注入尝试实际上会随着时间的推移，增强代理对恶意模式的识别能力。
正因为它的这个特性，如果将伪造的历史记录文本添加到上下文中，很难与真实的上下文进行区分。

### 间接内存注入

#### 构建间接内存注入

1. 不能只写一句命令，要伪装成一次完整的对话
2. 伪造的对话记录必须看起来是“已完成、已结束”的
3. 整个“伪造的日记”后面，必须跟一个真实、无害的新问题

#### 跨平台攻击

### 直接内存注入

# Benchmarking and Evaluating Context Ma-  nipulation Attacks with CrAIBench

## CrAIBench

将AgentDojo修改：增加了一个**模拟的持久化记忆系统**。当AI执行任务时，系统会用一个特定的提示词模板（`[memory] The following summary...`）将过去的操作历史（即“记忆”）喂给大语言模型。  
将这些动作归类到三个不同的环境中，用以模拟不同类型和严重程度的攻击：
- **链 (Chain) 环境**: 包含最基础的区块链操作（转账、跨链、质押等）。这里的注入攻击比较直接，说服性较低。
- **交易 (Trading) 环境**: 包含执行交易、下单等动作。这里的注入攻击更通用、更具说服力，看起来像是合理的交易建议。
- **DAO/NFT 环境**: 包含创建NFT、提交提案、投票等复杂交互。这里的攻击最具操纵性，可以非常微妙地模仿用户的真实意图，风险最高。

三大评估指标：
- **良性效用 (Benign Utility)**：在没有攻击的情况下，AI成功完成用户任务的比率。
- **受攻击下的效用 (Utility Under Attack)**：在遭受攻击时，AI仍然能正确执行用户原始意图的比率。（这个指标的反面，就代表了攻击在多大程度上干扰了正常功能）。
- **定向攻击成功率 (Targeted Attack Success Rate - ASR)**：在遭受攻击时，AI执行了**攻击者意图**的比率。

# Evaluating Context Manipulation Attacks

## 实验设置

测试对象：
- `GPT-4o-mini`
- `GPT-4o`
- `Claude Sonnet 3.5`
- `Claude Sonnet 3.7` 

攻击方法：
- **测试MI时**：启用模拟的记忆模块，并关闭PI的注入通道。
- **测试PI时**：关闭记忆模块，只使用AgentDojo框架原生的PI注入通道。
- **PI攻击**：使用了**最强**的攻击“包装”，比如用上用户的名字、AI模型的名称，并加上“重要消息”等字眼，让攻击指令看起来非常可信和有说服力。这相当于让PI攻击在“满级神装”的状态下进行。
- **MI攻击**：则**不假设攻击者有任何额外知识**，仅仅使用前文提到的中性、毫无说服力的“[memory]...”模板来注入虚假记忆。这相当于让MI攻击在“裸装”状态下进行。

防御方法：
- **默认 (Default)**：AgentDojo提供的标准系统提示词，没有任何明确的安全防御指令。
- **安全 (Secure)**：提醒AI代理不要执行从记忆或外部工具中获取的潜在有害指令。
- **确认 (Confirmation)**：一个专门针对区块链的防御，要求AI代理在执行任何金融交易前，必须明确地向用户请求确认。

实验主要结论：
1. **记忆注入的攻击效果远超传统提示注入。**
2. **传统的提示词防御对“记忆注入”几乎无效。**
3. **基于微调的防御策略展现出更强的鲁棒性。**

## Effectiveness of Memory Injection Attacks

- **所有模型无一幸免**：无论模型大小或能力如何，MI的攻击成功率（ASR）都**持续且显著地高于**PI。
- **模型越强，防御PI能力越强，但对MI依然脆弱**：研究发现，模型能力越强（以“良性任务成功率”为衡量标准），其抵抗PI攻击的能力也越强。对于最强的Claude 3.7，PI攻击的成功率几乎降到了零。
- **然而，对于MI攻击，即便是最强的Claude 3.7，攻击成功率依然高达55.1%！** 这说明，单纯提升模型的推理能力并不能有效防御MI，因为它利用的是AI对“自身历史”的信任，而非简单的逻辑漏洞。
## Evaluating Defenses

### System prompt defenses

### Fine-tuning-based defense

- 微调后，在面对MI攻击时，模型的**攻击成功率（ASR）从灾难性的85.1%直线下降到了几乎可以忽略不计的1.7%！**
- 同时，AI在受攻击时依然能正确完成用户任务的能力（Utility under attack）也从44.6%大幅提升到了85.1%。
- 相比之下，前面那些提示词防御手段最多只能减少约30%的攻击，并且还会损害AI的正常工作能力。

# Discussion

## Attacks Specific to DeFi Agents

## Potential Safeguards

- **方法一：硬编码地址白名单 (Hardcoded Whitelist)**
    - **做法**：只允许AI向预先批准的、写死在代码里的地址转账。
    - **优点**：能有效防止资金被转移到未经授权的陌生地址
    - **缺点**：**严重削弱了AI的自主性和实用性**。对于需要和大量新地址进行动态交互的合法用户来说，这种限制是繁琐且不切实际的。而且，白名单本身也可能成为被攻击的目标。
- **方法二：带外确认 (Out-of-Band Confirmation)**
    - **做法**：对于高风险操作，要求用户通过外部渠道（如邮件、手机短信）进行二次确认。
    - **优点**：增加了一层强大的安全保障。
    - **缺点**：**完全违背了发展“高度自动化”AI代理的初衷**。如果事事都需要人工确认，那AI的价值就大打折扣了。这只能作为最后的无奈之举。
- **方法三：训练具备“上下文感知”能力的模型 (The Real Solution)**
    - 这是作者最为推崇的、也与前文实验结论相呼应的**根本性解决方案**。
    - **核心思想**：与其给AI设置各种外部规则，不如从根本上训练它，让它**理解自己所处的“角色”和“情境”**。
    - **具体做法**：为DeFi代理注入一种**“信托责任 (fiduciary responsibility)”** 的意识，让它像一个专业的金融从业者或审计师一样思考，能够自主地权衡风险和回报，而不是盲目地执行指令。
    - 一个具备上下文感知能力的模型，无论接收到的信息是善意还是恶意的，都能更好地理解当前局势，从而做出更稳健、更负责任的决策。

## Memory Injection Attacks on General Purpose Agents

对于其他领域的大模型，MI也有着不错的成功率，且比PI效果更强。

# Conclusion
新的攻击方法：**内存注入**。这种攻击直接利用并污染AI代理的记忆系统，因为这些记忆常常在多次交互、甚至多个用户之间共享，所以一次成功的攻击可能导致持久的、跨平台的安全漏洞。  
现有的、基于提示词工程的防御方法（比如让AI小心、多确认）虽然能抵挡一些表层攻击，但对于能直接腐化AI所存储的“上下文”或“记忆”的高级攻击者来说，这些防御**基本上是无效的**。

解决策略
- **提升大语言模型的训练方法**：通过微调等技术，从根本上增强模型对抗攻击的稳健性，让它学会识别和忽略被污染的上下文。
- **设计有原则的内存管理系统**：在AI代理的系统架构层面下功夫，设计出能强制执行**严格隔离 (isolation)**和**完整性保证 (integrity guarantees)**的内存系统。这意味着从系统设计上确保一部分记忆不被另一部分污染，且记忆内容无法被轻易篡改。